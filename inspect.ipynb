{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to run the code\n",
    "\n",
    "```bash\n",
    "conda env create -f nlp_gpu.yml\n",
    "conda activate nlp_gpu\n",
    "```\n",
    "\n",
    "Example training command:\n",
    "```bash\n",
    "python train.py --model TreeLSTM --word_embeddings glove --trainable_embeddings --supervise_nodes --batch_size 128 --patience 10 --max_epochs 100\n",
    "```\n",
    "\n",
    "Results will be saved in the `results` folder, in json format.\n",
    "\n",
    "### Analysis\n",
    "\n",
    "Our results can be found in the `results` folder.\n",
    "To run the analysis, use the `analysis.ipynb` notebook, it generates the plots and tables in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import requests\n",
      "import os\n",
      "import zipfile\n",
      "import re\n",
      "from collections import Counter, OrderedDict, namedtuple\n",
      "from nltk import Tree\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "\n",
      "class OrderedCounter(Counter, OrderedDict):\n",
      "    \"\"\"Counter that remembers the order elements are first seen\"\"\"\n",
      "\n",
      "    def __repr__(self):\n",
      "        return \"%s(%r)\" % (self.__class__.__name__, OrderedDict(self))\n",
      "\n",
      "    def __reduce__(self):\n",
      "        return self.__class__, (OrderedDict(self),)\n",
      "\n",
      "\n",
      "class Vocabulary:\n",
      "    \"\"\"A vocabulary, assigns IDs to tokens\"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        self.freqs = OrderedCounter()\n",
      "        self.w2i = {}\n",
      "        self.i2w = []\n",
      "        self.i2t = [\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"]\n",
      "        self.t2i = OrderedDict({p : i for p, i in zip(self.i2t, range(len(self.i2t)))})\n",
      "\n",
      "    def count_token(self, t):\n",
      "        self.freqs[t] += 1\n",
      "\n",
      "    def add_token(self, t):\n",
      "        self.w2i[t] = len(self.w2i)\n",
      "        self.i2w.append(t)\n",
      "\n",
      "    def build(self, min_freq=0):\n",
      "        \"\"\"\n",
      "        min_freq: minimum number of occurrences for a word to be included\n",
      "                  in the vocabulary\n",
      "        \"\"\"\n",
      "        self.add_token(\"<unk>\")  # reserve 0 for <unk> (unknown words)\n",
      "        self.add_token(\"<pad>\")  # reserve 1 for <pad> (discussed later)\n",
      "\n",
      "        tok_freq = list(self.freqs.items())\n",
      "        tok_freq.sort(key=lambda x: x[1], reverse=True)\n",
      "        for tok, freq in tok_freq:\n",
      "            if freq >= min_freq:\n",
      "                self.add_token(tok)\n",
      "\n",
      "\n",
      "class SentimentDataset(Dataset):\n",
      "    def __init__(self, split=\"train\", lower=False, supervise_nodes=False):\n",
      "        self.split = split\n",
      "        self.lower = lower\n",
      "        self.supervise_nodes = supervise_nodes\n",
      "\n",
      "        # Download and extract data if needed\n",
      "        self._maybe_download_and_extract()\n",
      "\n",
      "        # Define constants\n",
      "        self.SHIFT = 0\n",
      "        self.REDUCE = 1\n",
      "\n",
      "        # Load data\n",
      "        self.Example = namedtuple(\"Example\", [\"tokens\", \"tree\", \"label\", \"transitions\"])\n",
      "        self.data = list(self._examplereader(f\"trees/{split}.txt\", lower=lower, supervise_nodes=supervise_nodes))\n",
      "\n",
      "        # Build vocabulary\n",
      "        self.vocab = self._build_vocabulary()\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)\n",
      "\n",
      "    def __getitem__(self, idx):\n",
      "        return self.data[idx]\n",
      "\n",
      "    def _maybe_download_and_extract(self):\n",
      "        url = \"http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip\"\n",
      "        zip_path = \"trainDevTestTrees_PTB.zip\"\n",
      "        extract_path = \"trees\"\n",
      "\n",
      "        if not os.path.exists(extract_path):\n",
      "            # Download if not exists\n",
      "            if not os.path.exists(zip_path):\n",
      "                print(\"Downloading sentiment dataset...\")\n",
      "                response = requests.get(url, stream=True)\n",
      "                response.raise_for_status()\n",
      "\n",
      "                with open(zip_path, \"wb\") as f:\n",
      "                    for chunk in response.iter_content(chunk_size=8192):\n",
      "                        f.write(chunk)\n",
      "\n",
      "                # Extract\n",
      "                print(\"Extracting files...\")\n",
      "                with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
      "                    zip_ref.extractall(extract_path)\n",
      "\n",
      "    def _filereader(self, path):\n",
      "        with open(path, mode=\"r\", encoding=\"utf-8\") as f:\n",
      "            for line in f:\n",
      "                yield line.strip().replace(\"\\\\\", \"\")\n",
      "\n",
      "    def _tokens_from_treestring(self, s):\n",
      "        \"\"\"Extract the tokens from a sentiment tree\"\"\"\n",
      "        return re.sub(r\"\\([0-9] |\\)\", \"\", s).split()\n",
      "\n",
      "    def _transitions_from_treestring(self, s):\n",
      "        s = re.sub(\"\\([0-5] ([^)]+)\\)\", \"0\", s)\n",
      "        s = re.sub(\"\\)\", \" )\", s)\n",
      "        s = re.sub(\"\\([0-4] \", \"\", s)\n",
      "        s = re.sub(\"\\([0-4] \", \"\", s)\n",
      "        s = re.sub(\"\\)\", \"1\", s)\n",
      "        return list(map(int, s.split()))\n",
      "    \n",
      "    def _extract_subtrees(self, tree):\n",
      "        subtrees_with_labels = []\n",
      "        for subtree in tree.subtrees():\n",
      "            subtrees_with_labels.append((subtree.label(), subtree))\n",
      "        return subtrees_with_labels\n",
      "\n",
      "    def _examplereader(self, path, lower=False, supervise_nodes=False):\n",
      "        \"\"\"Returns all examples in a file one by one.\"\"\"\n",
      "        for line in self._filereader(path):\n",
      "            line = line.lower() if lower else line\n",
      "            tokens = self._tokens_from_treestring(line)\n",
      "            tree = Tree.fromstring(line)\n",
      "            label = int(line[1])\n",
      "            trans = self._transitions_from_treestring(line)\n",
      "            yield self.Example(tokens=tokens, tree=tree, label=label, transitions=trans)\n",
      "            \n",
      "            if supervise_nodes:\n",
      "                subtrees_with_labels = self._extract_subtrees(tree)\n",
      "                for label, subtree in subtrees_with_labels:\n",
      "                    line_repr = str(subtree).replace('\\n', '')\n",
      "                    yield self.Example(\n",
      "                        tokens=self._tokens_from_treestring(line_repr),\n",
      "                        tree=subtree,\n",
      "                        label=int(label), \n",
      "                        transitions=self._transitions_from_treestring(line_repr)\n",
      "                    )\n",
      "\n",
      "    def _build_vocabulary(self):\n",
      "        vocab = Vocabulary()\n",
      "        for example in self.data:\n",
      "            for token in example.tokens:\n",
      "                vocab.count_token(token)\n",
      "        vocab.build()\n",
      "        return vocab\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import data\n",
    "print(inspect.getsource(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class BOW(nn.Module):\n",
      "    \"\"\"A simple bag-of-words model\"\"\"\n",
      "\n",
      "    def __init__(self, vocab_size, embedding_dim, vocab):\n",
      "        super(BOW, self).__init__()\n",
      "        self.vocab = vocab\n",
      "\n",
      "        # this is a trainable look-up table with word embeddings\n",
      "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
      "\n",
      "        # this is a trainable bias term\n",
      "        self.bias = nn.Parameter(torch.zeros(embedding_dim), requires_grad=True)\n",
      "\n",
      "    def forward(self, inputs):\n",
      "        # this is the forward pass of the neural network\n",
      "        # it applies a function to the input and returns the output\n",
      "\n",
      "        # this looks up the embeddings for each word ID in inputs\n",
      "        # the result is a sequence of word embeddings\n",
      "        embeds = self.embed(inputs)\n",
      "\n",
      "        # the output is the sum across the time dimension (1)\n",
      "        # with the bias term added\n",
      "        logits = embeds.sum(1) + self.bias\n",
      "\n",
      "        return logits\n",
      "\n",
      "class CBOW(nn.Module):\n",
      "    \"\"\"A continuous bag-of-words model\"\"\"\n",
      "\n",
      "    def __init__(self, vocab_size, embedding_dim, n_classes, vocab):\n",
      "        super(CBOW, self).__init__()\n",
      "        self.vocab = vocab\n",
      "\n",
      "        # Embedding layer\n",
      "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
      "\n",
      "        # Linear layer\n",
      "        self.fc = nn.Linear(embedding_dim, n_classes)\n",
      "\n",
      "    def forward(self, inputs):\n",
      "        # Get embeddings\n",
      "        embeds = self.embed(inputs)  # (batch_size, sequence_length, embedding_dim)\n",
      "        # Sum over sequence length\n",
      "        embeds_sum = embeds.sum(dim=1)  # (batch_size, embedding_dim)\n",
      "        # Linear projection\n",
      "        logits = self.fc(embeds_sum)  # (batch_size, n_classes)\n",
      "        return logits\n",
      "\n",
      "class Deep_CBOW(nn.Module):\n",
      "    \"\"\"A deep continuous bag-of-words model\"\"\"\n",
      "\n",
      "    def __init__(self, vocab_size, embedding_dim, hidden_layer, n_classes, vocab):\n",
      "        super(Deep_CBOW, self).__init__()\n",
      "        self.vocab = vocab\n",
      "\n",
      "        # Embedding layer\n",
      "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
      "\n",
      "        # Deep neural network for output layer\n",
      "        self.output_layer = nn.Sequential(\n",
      "            nn.Linear(embedding_dim, hidden_layer),  # E -> D\n",
      "            nn.Tanh(),\n",
      "            nn.Linear(hidden_layer, hidden_layer),  # D -> D\n",
      "            nn.Tanh(),\n",
      "            nn.Linear(hidden_layer, n_classes),  # D -> C\n",
      "        )\n",
      "\n",
      "    def forward(self, inputs):\n",
      "        # Get embeddings\n",
      "        embeds = self.embed(inputs)  # (batch_size, sequence_length, embedding_dim)\n",
      "\n",
      "        # Sum over sequence length\n",
      "        embeds_sum = embeds.sum(dim=1)  # (batch_size, embedding_dim)\n",
      "\n",
      "        # Pass through the output layer\n",
      "        logits = self.output_layer(embeds_sum)\n",
      "\n",
      "        return logits\n",
      "\n",
      "class PTDeepCBOW(Deep_CBOW):\n",
      "    def __init__(\n",
      "        self,\n",
      "        vocab_size,\n",
      "        embedding_dim,\n",
      "        hidden_dim,\n",
      "        output_dim,\n",
      "        vocab,\n",
      "        pretrained_vectors,\n",
      "        trainable_embeddings=False,\n",
      "    ):\n",
      "        super(PTDeepCBOW, self).__init__(\n",
      "            vocab_size, embedding_dim, hidden_dim, output_dim, vocab\n",
      "        )\n",
      "\n",
      "        # Load pre-trained embeddings and freeze them\n",
      "        self.embed = nn.Embedding.from_pretrained(\n",
      "            embeddings=torch.tensor(pretrained_vectors, dtype=torch.float32),\n",
      "            freeze=not trainable_embeddings,  # Freeze embeddings to prevent updates\n",
      "        )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.bow import BOW, CBOW, Deep_CBOW, PTDeepCBOW\n",
    "print(inspect.getsource(BOW))\n",
    "print(inspect.getsource(CBOW))\n",
    "print(inspect.getsource(Deep_CBOW))\n",
    "print(inspect.getsource(PTDeepCBOW))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class LSTMClassifier(nn.Module):\n",
      "    \"\"\"Encodes sentence with an LSTM and projects final hidden state\"\"\"\n",
      "\n",
      "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, vocab):\n",
      "        super(LSTMClassifier, self).__init__()\n",
      "        self.vocab = vocab\n",
      "        self.hidden_dim = hidden_dim\n",
      "        self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
      "        self.rnn = MyLSTMCell(embedding_dim, hidden_dim)\n",
      "\n",
      "        self.output_layer = nn.Sequential(\n",
      "            nn.Dropout(p=0.5), nn.Linear(hidden_dim, output_dim)  # explained later\n",
      "        )\n",
      "\n",
      "    def forward(self, x):\n",
      "\n",
      "        B = x.size(0)  # batch size (this is 1 for now, i.e. 1 single example)\n",
      "        T = x.size(1)  # timesteps (the number of words in the sentence)\n",
      "\n",
      "        input_ = self.embed(x)\n",
      "\n",
      "        # here we create initial hidden states containing zeros\n",
      "        # we use a trick here so that, if input is on the GPU, then so are hx and cx\n",
      "        hx = input_.new_zeros(B, self.rnn.hidden_size)\n",
      "        cx = input_.new_zeros(B, self.rnn.hidden_size)\n",
      "\n",
      "        # process input sentences one word/timestep at a time\n",
      "        # input is batch-major (i.e., batch size is the first dimension)\n",
      "        # so the first word(s) is (are) input_[:, 0]\n",
      "        outputs = []\n",
      "        for i in range(T):\n",
      "            hx, cx = self.rnn(input_[:, i], (hx, cx))\n",
      "            outputs.append(hx)\n",
      "\n",
      "        # if we have a single example, our final LSTM state is the last hx\n",
      "        if B == 1:\n",
      "            final = hx\n",
      "        else:\n",
      "            #\n",
      "            # This part is explained in next section, ignore this else-block for now.\n",
      "            #\n",
      "            # We processed sentences with different lengths, so some of the sentences\n",
      "            # had already finished and we have been adding padding inputs to hx.\n",
      "            # We select the final state based on the length of each sentence.\n",
      "\n",
      "            # two lines below not needed if using LSTM from pytorch\n",
      "            outputs = torch.stack(outputs, dim=0)  # [T, B, D]\n",
      "            outputs = outputs.transpose(0, 1).contiguous()  # [B, T, D]\n",
      "\n",
      "            # to be super-sure we're not accidentally indexing the wrong state\n",
      "            # we zero out positions that are invalid\n",
      "            pad_positions = (x == 1).unsqueeze(-1)\n",
      "\n",
      "            outputs = outputs.contiguous()\n",
      "            outputs = outputs.masked_fill_(pad_positions, 0.0)\n",
      "\n",
      "            mask = x != 1  # true for valid positions [B, T]\n",
      "            lengths = mask.sum(dim=1)  # [B, 1]\n",
      "\n",
      "            indexes = (lengths - 1) + torch.arange(\n",
      "                B, device=x.device, dtype=x.dtype\n",
      "            ) * T\n",
      "            final = outputs.view(-1, self.hidden_dim)[indexes]  # [B, D]\n",
      "\n",
      "        # we use the last hidden state to classify the sentence\n",
      "        logits = self.output_layer(final)\n",
      "        return logits\n",
      "\n",
      "class MyLSTMCell(nn.Module):\n",
      "    \"\"\"Our own LSTM cell\"\"\"\n",
      "\n",
      "    def __init__(self, input_size, hidden_size, bias=True):\n",
      "        \"\"\"Creates the weights for this LSTM\"\"\"\n",
      "        super(MyLSTMCell, self).__init__()\n",
      "\n",
      "        self.input_size = input_size\n",
      "        self.hidden_size = hidden_size\n",
      "        self.bias = bias\n",
      "\n",
      "        # YOUR CODE HERE\n",
      "        # Input gate weights\n",
      "        self.W_i = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
      "        self.U_i = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
      "        if self.bias:\n",
      "            self.b_i = nn.Parameter(torch.Tensor(hidden_size))\n",
      "        else:\n",
      "            self.register_parameter(\"b_i\", None)\n",
      "\n",
      "        # Forget gate weights\n",
      "        self.W_f = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
      "        self.U_f = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
      "        if self.bias:\n",
      "            self.b_f = nn.Parameter(torch.Tensor(hidden_size))\n",
      "        else:\n",
      "            self.register_parameter(\"b_f\", None)\n",
      "\n",
      "        # Cell gate weights\n",
      "        self.W_g = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
      "        self.U_g = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
      "        if self.bias:\n",
      "            self.b_g = nn.Parameter(torch.Tensor(hidden_size))\n",
      "        else:\n",
      "            self.register_parameter(\"b_g\", None)\n",
      "\n",
      "        # Output gate weights\n",
      "        self.W_o = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
      "        self.U_o = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
      "        if self.bias:\n",
      "            self.b_o = nn.Parameter(torch.Tensor(hidden_size))\n",
      "        else:\n",
      "            self.register_parameter(\"b_o\", None)\n",
      "\n",
      "        self.reset_parameters()\n",
      "\n",
      "    def reset_parameters(self):\n",
      "        \"\"\"This is PyTorch's default initialization method\"\"\"\n",
      "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
      "        for weight in self.parameters():\n",
      "            weight.data.uniform_(-stdv, stdv)\n",
      "\n",
      "    def forward(self, input_, hx, mask=None):\n",
      "        \"\"\"\n",
      "        input is (batch, input_size)\n",
      "        hx is ((batch, hidden_size), (batch, hidden_size))\n",
      "        \"\"\"\n",
      "        prev_h, prev_c = hx\n",
      "\n",
      "        W_all = torch.cat([self.W_i, self.W_f, self.W_g, self.W_o], dim=1)\n",
      "        U_all = torch.cat([self.U_i, self.U_f, self.U_g, self.U_o], dim=1)\n",
      "        b_all = torch.cat([self.b_i, self.b_f, self.b_g, self.b_o], dim=0)\n",
      "\n",
      "        Wx_plus_Uh = input_ @ W_all + prev_h @ U_all + b_all\n",
      "        i, f, g, o = Wx_plus_Uh.chunk(4, dim=1)\n",
      "\n",
      "        i = torch.sigmoid(i)\n",
      "        f = torch.sigmoid(f)\n",
      "        g = torch.tanh(g)\n",
      "        o = torch.sigmoid(o)\n",
      "\n",
      "        # Compute new cell state\n",
      "        c = f * prev_c + i * g\n",
      "\n",
      "        # Compute new hidden state\n",
      "        h = o * torch.tanh(c)\n",
      "\n",
      "        return h, c\n",
      "\n",
      "    def __repr__(self):\n",
      "        return \"{}({:d}, {:d})\".format(\n",
      "            self.__class__.__name__, self.input_size, self.hidden_size\n",
      "        )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.lstm import LSTMClassifier, MyLSTMCell\n",
    "print(inspect.getsource(LSTMClassifier))\n",
    "print(inspect.getsource(MyLSTMCell))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TreeLSTM N-ary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class TreeLSTMClassifier(nn.Module):\n",
      "    \"\"\"Encodes sentence with a TreeLSTM and projects final hidden state\"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        vocab_size,\n",
      "        embedding_dim,\n",
      "        hidden_dim,\n",
      "        output_dim,\n",
      "        vocab,\n",
      "        reduce_fn=TreeLSTMCell,\n",
      "    ):\n",
      "        super(TreeLSTMClassifier, self).__init__()\n",
      "        self.vocab = vocab\n",
      "        self.hidden_dim = hidden_dim\n",
      "        self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx=1)\n",
      "        self.treelstm = TreeLSTM(embedding_dim, hidden_dim, reduce_fn=reduce_fn)\n",
      "        self.output_layer = nn.Sequential(\n",
      "            nn.Dropout(p=0.5), nn.Linear(hidden_dim, output_dim, bias=True)\n",
      "        )\n",
      "\n",
      "    def forward(self, x):\n",
      "\n",
      "        # x is a pair here of words and transitions; we unpack it here.\n",
      "        # x is batch-major: [B, T], transitions is time major [2T-1, B]\n",
      "        x, transitions = x\n",
      "        emb = self.embed(x)\n",
      "\n",
      "        # we use the root/top state of the Tree LSTM to classify the sentence\n",
      "        root_states = self.treelstm(emb, transitions)\n",
      "\n",
      "        # we use the last hidden state to classify the sentence\n",
      "        logits = self.output_layer(root_states)\n",
      "        return logits\n",
      "\n",
      "class TreeLSTM(nn.Module):\n",
      "    \"\"\"Encodes a sentence using a TreeLSTMCell\"\"\"\n",
      "\n",
      "    def __init__(self, input_size, hidden_size, bias=True, reduce_fn=TreeLSTMCell):\n",
      "        \"\"\"Creates the weights for this LSTM\"\"\"\n",
      "        super(TreeLSTM, self).__init__()\n",
      "\n",
      "        self.input_size = input_size\n",
      "        self.hidden_size = hidden_size\n",
      "        self.bias = bias\n",
      "        self.SHIFT = 0\n",
      "        self.REDUCE = 1\n",
      "        self.reduce = reduce_fn(input_size, hidden_size)\n",
      "\n",
      "        # project word to initial c\n",
      "        self.proj_x = nn.Linear(input_size, hidden_size)\n",
      "        self.proj_x_gate = nn.Linear(input_size, hidden_size)\n",
      "\n",
      "        self.buffers_dropout = nn.Dropout(p=0.5)\n",
      "\n",
      "    def forward(self, x, transitions):\n",
      "        \"\"\"\n",
      "        WARNING: assuming x is reversed!\n",
      "        :param x: word embeddings [B, T, E]\n",
      "        :param transitions: [2T-1, B]\n",
      "        :return: root states\n",
      "        \"\"\"\n",
      "\n",
      "        B = x.size(0)  # batch size\n",
      "        T = x.size(1)  # time\n",
      "\n",
      "        # compute an initial c and h for each word\n",
      "        # Note: this corresponds to input x in the Tai et al. Tree LSTM paper.\n",
      "        # We do not handle input x in the TreeLSTMCell itself.\n",
      "        buffers_c = self.proj_x(x)\n",
      "        buffers_h = buffers_c.tanh()\n",
      "        buffers_h_gate = self.proj_x_gate(x).sigmoid()\n",
      "        buffers_h = buffers_h_gate * buffers_h\n",
      "\n",
      "        # concatenate h and c for each word\n",
      "        buffers = torch.cat([buffers_h, buffers_c], dim=-1)\n",
      "\n",
      "        D = buffers.size(-1) // 2\n",
      "\n",
      "        # we turn buffers into a list of stacks (1 stack for each sentence)\n",
      "        # first we split buffers so that it is a list of sentences (length B)\n",
      "        # then we split each sentence to be a list of word vectors\n",
      "        buffers = buffers.split(1, dim=0)  # Bx[T, 2D]\n",
      "        buffers = [list(b.squeeze(0).split(1, dim=0)) for b in buffers]  # BxTx[2D]\n",
      "\n",
      "        # create B empty stacks\n",
      "        stacks = [[] for _ in buffers]\n",
      "\n",
      "        # t_batch holds 1 transition for each sentence\n",
      "        for t_batch in transitions:\n",
      "\n",
      "            child_l = []  # contains the left child for each sentence with reduce action\n",
      "            child_r = []  # contains the corresponding right child\n",
      "\n",
      "            # iterate over sentences in the batch\n",
      "            # each has a transition t, a buffer and a stack\n",
      "            for transition, buffer, stack in zip(t_batch, buffers, stacks):\n",
      "                if transition == self.SHIFT:\n",
      "                    stack.append(buffer.pop())\n",
      "                elif transition == self.REDUCE:\n",
      "                    assert (\n",
      "                        len(stack) >= 2\n",
      "                    ), \"Stack too small! Should not happen with valid transition sequences\"\n",
      "                    child_r.append(stack.pop())  # right child is on top\n",
      "                    child_l.append(stack.pop())\n",
      "\n",
      "            # if there are sentences with reduce transition, perform them batched\n",
      "            if child_l:\n",
      "                reduced = iter(\n",
      "                    utils.unbatch(\n",
      "                        self.reduce(utils.batch(child_l), utils.batch(child_r))\n",
      "                    )\n",
      "                )\n",
      "                for transition, stack in zip(t_batch, stacks):\n",
      "                    if transition == self.REDUCE:\n",
      "                        stack.append(next(reduced))\n",
      "\n",
      "        final = [stack.pop().chunk(2, -1)[0] for stack in stacks]\n",
      "        final = torch.cat(final, dim=0)  # tensor [B, D]\n",
      "\n",
      "        return final\n",
      "\n",
      "class TreeLSTMCell(nn.Module):\n",
      "    \"\"\"A Binary Tree LSTM cell\"\"\"\n",
      "\n",
      "    def __init__(self, input_size, hidden_size, bias=True):\n",
      "        \"\"\"Creates the weights for this LSTM\"\"\"\n",
      "        super(TreeLSTMCell, self).__init__()\n",
      "\n",
      "        self.input_size = input_size\n",
      "        self.hidden_size = hidden_size\n",
      "        self.bias = bias\n",
      "\n",
      "        self.reduce_layer = nn.Linear(2 * hidden_size, 5 * hidden_size)\n",
      "        self.dropout_layer = nn.Dropout(p=0.25)\n",
      "\n",
      "        self.reset_parameters()\n",
      "\n",
      "    def reset_parameters(self):\n",
      "        \"\"\"This is PyTorch's default initialization method\"\"\"\n",
      "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
      "        for weight in self.parameters():\n",
      "            weight.data.uniform_(-stdv, stdv)\n",
      "\n",
      "    def forward(self, hx_l, hx_r, mask=None):\n",
      "        \"\"\"\n",
      "        hx_l is ((batch, hidden_size), (batch, hidden_size))\n",
      "        hx_r is ((batch, hidden_size), (batch, hidden_size))\n",
      "        \"\"\"\n",
      "        prev_h_l, prev_c_l = hx_l  # left child\n",
      "        prev_h_r, prev_c_r = hx_r  # right child\n",
      "\n",
      "        B = prev_h_l.size(0)\n",
      "\n",
      "        # we concatenate the left and right children\n",
      "        # you can also project from them separately and then sum\n",
      "        children = torch.cat([prev_h_l, prev_h_r], dim=1)\n",
      "\n",
      "        # project the combined children into a 5D tensor for i,fl,fr,g,o\n",
      "        # this is done for speed, and you could also do it separately\n",
      "        proj = self.reduce_layer(children)  # shape: B x 5D\n",
      "\n",
      "        # each shape: B x D\n",
      "        i, f_l, f_r, g, o = torch.chunk(proj, 5, dim=-1)\n",
      "\n",
      "        # main Tree LSTM computation\n",
      "\n",
      "        # YOUR CODE HERE\n",
      "        # You only need to complete the commented lines below.\n",
      "        # raise NotImplementedError(\"Implement this.\")\n",
      "\n",
      "        # The shape of each of these is [batch_size, hidden_size]\n",
      "\n",
      "        i = torch.sigmoid(i)\n",
      "        f_l = torch.sigmoid(f_l)\n",
      "        f_r = torch.sigmoid(f_r)\n",
      "        g = torch.tanh(g)\n",
      "        o = torch.sigmoid(o)\n",
      "\n",
      "        c = i * g + f_l * prev_c_l + f_r * prev_c_r\n",
      "        h = o * torch.tanh(c)\n",
      "\n",
      "        return h, c\n",
      "\n",
      "    def __repr__(self):\n",
      "        return \"{}({:d}, {:d})\".format(\n",
      "            self.__class__.__name__, self.input_size, self.hidden_size\n",
      "        )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.lstm import TreeLSTMCell, TreeLSTM, TreeLSTMClassifier\n",
    "print(inspect.getsource(TreeLSTMClassifier))\n",
    "print(inspect.getsource(TreeLSTM))\n",
    "print(inspect.getsource(TreeLSTMCell))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TreeLSTM Child-Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ChildSumTreeLSTMCell(nn.Module):\n",
      "    \"\"\"A child-sum Tree LSTM cell\"\"\"\n",
      "\n",
      "    def __init__(self, input_size, hidden_size, bias=True):\n",
      "        \"\"\"Creates the weights for this LSTM\"\"\"\n",
      "        super(ChildSumTreeLSTMCell, self).__init__()\n",
      "\n",
      "        self.input_size = input_size\n",
      "        self.hidden_size = hidden_size\n",
      "        self.bias = bias\n",
      "\n",
      "        self.reduce_layer_1 = nn.Linear(hidden_size, 3 * hidden_size)\n",
      "        self.reduce_layer_forget = nn.Linear(hidden_size, hidden_size)\n",
      "        self.dropout_layer = nn.Dropout(p=0.25)\n",
      "\n",
      "        self.reset_parameters()\n",
      "\n",
      "    def reset_parameters(self):\n",
      "        \"\"\"This is PyTorch's default initialization method\"\"\"\n",
      "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
      "        for weight in self.parameters():\n",
      "            weight.data.uniform_(-stdv, stdv)\n",
      "\n",
      "    def forward(self, hx_l, hx_r, mask=None):\n",
      "        \"\"\"\n",
      "        hx_l is ((batch, hidden_size), (batch, hidden_size))\n",
      "        hx_r is ((batch, hidden_size), (batch, hidden_size))\n",
      "        \"\"\"\n",
      "        prev_h_l, prev_c_l = hx_l  # left child\n",
      "        prev_h_r, prev_c_r = hx_r  # right child\n",
      "\n",
      "        B = prev_h_l.size(0)\n",
      "\n",
      "        # you can also project from them separately and then sum\n",
      "        children = prev_h_l + prev_h_r\n",
      "\n",
      "        # project the combined children into a 5D tensor for i,fl,fr,g,o\n",
      "        # this is done for speed, and you could also do it separately\n",
      "        proj = self.reduce_layer_1(children)  # shape: B x 3D\n",
      "\n",
      "        # each shape: B x D\n",
      "        i, g, o = torch.chunk(proj, 3, dim=-1)\n",
      "\n",
      "        proj_forget = self.reduce_layer_forget(torch.cat([prev_h_l, prev_h_r], dim=0))\n",
      "\n",
      "        # each shape: B x D\n",
      "        f_l, f_r = torch.chunk(proj_forget, 2, dim=0)\n",
      "\n",
      "        # main Tree LSTM computation\n",
      "\n",
      "        # YOUR CODE HERE\n",
      "        # You only need to complete the commented lines below.\n",
      "        # raise NotImplementedError(\"Implement this.\")\n",
      "\n",
      "        # The shape of each of these is [batch_size, hidden_size]\n",
      "\n",
      "        i = torch.sigmoid(i)\n",
      "        f_l = torch.sigmoid(f_l)\n",
      "        f_r = torch.sigmoid(f_r)\n",
      "        g = torch.tanh(g)\n",
      "        o = torch.sigmoid(o)\n",
      "\n",
      "        c = i * g + f_l * prev_c_l + f_r * prev_c_r\n",
      "        h = o * torch.tanh(c)\n",
      "\n",
      "        return h, c\n",
      "\n",
      "    def __repr__(self):\n",
      "        return \"{}({:d}, {:d})\".format(\n",
      "            self.__class__.__name__, self.input_size, self.hidden_size\n",
      "        )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.lstm import ChildSumTreeLSTMCell\n",
    "print(inspect.getsource(ChildSumTreeLSTMCell))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def train_model(\n",
      "    model,\n",
      "    optimizer,\n",
      "    train_data,\n",
      "    dev_data,\n",
      "    test_data,\n",
      "    device,\n",
      "    max_epochs=100,\n",
      "    eval_every=1,\n",
      "    batch_fn=None,\n",
      "    prep_fn=None,\n",
      "    eval_fn=None,\n",
      "    batch_size=None,\n",
      "    patience=5,\n",
      "):\n",
      "\n",
      "    criterion = nn.CrossEntropyLoss()\n",
      "    train_epoch_losses = []\n",
      "    val_epoch_metrics = []\n",
      "\n",
      "    # early stopping\n",
      "    best_val_accuracy = float(\"-inf\")\n",
      "    best_epoch = 0\n",
      "    patience_counter = 0\n",
      "    best_model = None\n",
      "    try:\n",
      "        for epoch in range(max_epochs):\n",
      "            train_loss = 0\n",
      "            num_batches = 0\n",
      "            for batch in tqdm(\n",
      "                batch_fn(train_data.data, batch_size=batch_size, shuffle=True)\n",
      "            ):\n",
      "                num_batches += 1\n",
      "\n",
      "                # Forward pass\n",
      "                model.train()\n",
      "                x, targets = prep_fn(batch, model.vocab, device=device)\n",
      "\n",
      "                logits = model(x)\n",
      "\n",
      "                B = targets.size(0)  # Batch size\n",
      "\n",
      "                # Compute cross-entropy loss\n",
      "                loss = criterion(logits.view([B, -1]), targets.view(-1))\n",
      "                train_loss += loss.item()\n",
      "\n",
      "                # Backward pass and optimization step\n",
      "                optimizer.zero_grad()\n",
      "                loss.backward()\n",
      "                optimizer.step()\n",
      "\n",
      "            train_epoch_losses.append(train_loss / num_batches)\n",
      "\n",
      "            # validation loop\n",
      "            if epoch % eval_every == 0:\n",
      "                with torch.no_grad():\n",
      "                    model.eval()\n",
      "                    val_metrics = eval_fn(\n",
      "                        model,\n",
      "                        dev_data.data,\n",
      "                        device=device,\n",
      "                        criterion=criterion,\n",
      "                        prep_fn=prep_fn,\n",
      "                        batch_size=batch_size,\n",
      "                        batch_fn=batch_fn,\n",
      "                    )\n",
      "                    val_epoch_metrics.append(val_metrics)\n",
      "\n",
      "                print(\n",
      "                    f\"Epoch {epoch}, train loss: {train_epoch_losses[-1]}, \"\n",
      "                    f\"val accuracy: {val_metrics['accuracy']}\"\n",
      "                )\n",
      "\n",
      "                if val_metrics['accuracy'] > best_val_accuracy:\n",
      "                    best_val_accuracy = val_metrics['accuracy']\n",
      "                    best_epoch = epoch\n",
      "                    patience_counter = 0\n",
      "                    best_model = copy.deepcopy(model)\n",
      "                else:\n",
      "                    patience_counter += 1\n",
      "                    print(f\"Patience counter: {patience_counter}\")\n",
      "                    if patience_counter >= patience:\n",
      "                        print(f\"Early stopping at epoch {epoch}\")\n",
      "                        break\n",
      "\n",
      "        # Evaluate on test set using best model\n",
      "        with torch.no_grad():\n",
      "            best_model.eval()\n",
      "            \n",
      "            # Group test data by sentence length\n",
      "            length_bins = {}\n",
      "            for example in test_data.data:\n",
      "                sent_len = len(example.tokens)\n",
      "                # Create bins of size 5 (1-5, 6-10, etc)\n",
      "                bin_idx = (sent_len - 1) // 5\n",
      "                if bin_idx not in length_bins:\n",
      "                    length_bins[bin_idx] = []\n",
      "                length_bins[bin_idx].append(example)\n",
      "            \n",
      "            # Evaluate each length bin separately\n",
      "            binned_metrics = {}\n",
      "            for bin_idx in sorted(length_bins.keys()):\n",
      "                start_len = bin_idx * 5 + 1\n",
      "                end_len = (bin_idx + 1) * 5\n",
      "                bin_data = length_bins[bin_idx]\n",
      "                \n",
      "                bin_metrics = eval_fn(\n",
      "                    best_model,\n",
      "                    bin_data,\n",
      "                    device=device,\n",
      "                    criterion=criterion,\n",
      "                    prep_fn=prep_fn,\n",
      "                    batch_size=batch_size,\n",
      "                    batch_fn=batch_fn,\n",
      "                )\n",
      "                \n",
      "                binned_metrics[f\"bin_{bin_idx}_{start_len}-{end_len}\"] = {\n",
      "                    \"metrics\": bin_metrics,\n",
      "                    \"num_examples\": len(bin_data)\n",
      "                }\n",
      "            \n",
      "            # Also compute overall metrics\n",
      "            test_metrics = eval_fn(\n",
      "                best_model,\n",
      "                test_data.data,\n",
      "                device=device,\n",
      "                criterion=criterion,\n",
      "                prep_fn=prep_fn,\n",
      "                batch_size=batch_size,\n",
      "                batch_fn=batch_fn,\n",
      "            )\n",
      "            \n",
      "            print(f\"Overall test metrics: {test_metrics}\")\n",
      "            print(\"\\nMetrics by sentence length:\")\n",
      "            for bin_name, bin_results in binned_metrics.items():\n",
      "                print(f\"\\n{bin_name} (n={bin_results['num_examples']}):\")\n",
      "                print(f\"Metrics: {bin_results['metrics']}\")\n",
      "\n",
      "    except KeyboardInterrupt:\n",
      "        print(\"Interrupted\")\n",
      "\n",
      "    return train_epoch_losses, val_epoch_metrics, test_metrics, binned_metrics, epoch\n",
      "\n",
      "def evaluate_metrics_extended_batch(\n",
      "    model, data, criterion, device, batch_fn=None, prep_fn=None, batch_size=16\n",
      "):\n",
      "    \"\"\"\n",
      "    Evaluates a model on the given dataset and returns a dictionary of metrics.\n",
      "    \"\"\"\n",
      "    model.eval()\n",
      "\n",
      "    correct = 0\n",
      "    total = 0\n",
      "    y_true = []\n",
      "    y_pred = []\n",
      "    loss = 0\n",
      "    num_batches = 0\n",
      "    with torch.no_grad():  # Disable gradient computation\n",
      "        for mb in batch_fn(data, batch_size=batch_size, shuffle=False):\n",
      "            num_batches += 1\n",
      "            # Preprocess the mini-batch to get input tensors and target labels\n",
      "            x, targets = prep_fn(mb, model.vocab, device=device)\n",
      "\n",
      "            # Forward pass: compute logits\n",
      "            logits = model(x)\n",
      "            B = targets.size(0)\n",
      "            loss += criterion(logits.view([B, -1]), targets.view(-1)).item()\n",
      "\n",
      "            # Get the predicted classes (as integers)\n",
      "            predictions = logits.argmax(dim=-1).view(-1)\n",
      "\n",
      "            # Update counters for accuracy\n",
      "            correct += (predictions == targets.view(-1)).sum().item()\n",
      "            total += targets.size(0)\n",
      "\n",
      "            # Append true and predicted labels for F1 score computation\n",
      "            y_true.extend(targets.view(-1).tolist())\n",
      "            y_pred.extend(predictions.tolist())\n",
      "\n",
      "    loss = loss / num_batches\n",
      "\n",
      "    # Calculate all metrics\n",
      "    accuracy = correct / total if total > 0 else 0.0\n",
      "    weighted_f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
      "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
      "\n",
      "    # Return metrics as a dictionary\n",
      "    metrics = {\n",
      "        \"loss\": loss / num_batches,\n",
      "        \"accuracy\": accuracy,\n",
      "        \"weighted_f1\": weighted_f1,\n",
      "        \"macro_f1\": macro_f1,\n",
      "    }\n",
      "\n",
      "    return metrics\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import (\n",
    "    train_model,\n",
    "    evaluate_metrics_extended_batch,\n",
    "    print_parameters,\n",
    "    get_minibatch,\n",
    "    prepare_minibatch,\n",
    "    prepare_treelstm_minibatch,\n",
    "    pad,\n",
    "    batch,\n",
    "    unbatch,\n",
    "    download_file,\n",
    "    load_pretrained_embeddings,\n",
    "    create_vocabulary_and_embeddings,\n",
    ")\n",
    "\n",
    "# these are the important ones\n",
    "print(inspect.getsource(train_model))\n",
    "print(inspect.getsource(evaluate_metrics_extended_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main (Training Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def main(args):\n",
      "\n",
      "    seeds = [1, 42, 1337]\n",
      "\n",
      "    # load data\n",
      "    train_dataset = SentimentDataset(\n",
      "        split=\"train\", lower=False, supervise_nodes=args.supervise_nodes\n",
      "    )\n",
      "    dev_dataset = SentimentDataset(\n",
      "        split=\"dev\", lower=False, supervise_nodes=False\n",
      "    )  # keep dev and test as is\n",
      "    test_dataset = SentimentDataset(split=\"test\", lower=False, supervise_nodes=False)\n",
      "\n",
      "    results_dir = \"results\"\n",
      "    os.makedirs(results_dir, exist_ok=True)\n",
      "\n",
      "    train_epoch_losses_list = []\n",
      "    val_epoch_metrics_list = []\n",
      "    test_metrics_list = []\n",
      "    binned_metrics_list = []\n",
      "    max_epochs = []\n",
      "    for seed in seeds:\n",
      "\n",
      "        # Set random seeds for reproducibility\n",
      "        torch.manual_seed(seed)\n",
      "        torch.cuda.manual_seed(seed)\n",
      "        np.random.seed(seed)\n",
      "        random.seed(seed)\n",
      "\n",
      "        # initialize model\n",
      "        if args.model == \"BOW\":\n",
      "            vocab_size = len(train_dataset.vocab.w2i)\n",
      "            n_classes = len(train_dataset.vocab.t2i)\n",
      "            model = bow.BOW(vocab_size, n_classes, train_dataset.vocab)\n",
      "            model = model.to(device)\n",
      "            print(model)\n",
      "            utils.print_parameters(model)\n",
      "            optimizer = optim.Adam(\n",
      "                model.parameters(),\n",
      "                lr=0.01,\n",
      "                weight_decay=0.0001,\n",
      "                betas=(0.9, 0.999),\n",
      "                eps=1e-08,\n",
      "                amsgrad=True,\n",
      "            )\n",
      "            prep_fn = utils.prepare_minibatch\n",
      "        elif args.model == \"CBOW\":\n",
      "            embedding_dim = 300\n",
      "            n_classes = len(train_dataset.vocab.t2i)\n",
      "            model = bow.CBOW(\n",
      "                vocab_size=len(train_dataset.vocab.w2i),\n",
      "                embedding_dim=embedding_dim,\n",
      "                n_classes=n_classes,\n",
      "                vocab=train_dataset.vocab,\n",
      "            )\n",
      "            model = model.to(device)\n",
      "            print(model)\n",
      "            utils.print_parameters(model)\n",
      "            optimizer = optim.Adam(\n",
      "                model.parameters(),\n",
      "                lr=0.0001,\n",
      "                weight_decay=1e-5,\n",
      "                eps=1e-08,\n",
      "                amsgrad=False,\n",
      "            )\n",
      "            prep_fn = utils.prepare_minibatch\n",
      "        elif args.model == \"DeepCBOW\":\n",
      "            embedding_dim = 300\n",
      "            n_classes = len(train_dataset.vocab.t2i)\n",
      "            hidden_layer = 100\n",
      "            vocab_size = len(train_dataset.vocab.w2i)\n",
      "            model = bow.Deep_CBOW(\n",
      "                vocab_size,\n",
      "                embedding_dim,\n",
      "                hidden_layer,\n",
      "                n_classes,\n",
      "                vocab=train_dataset.vocab,\n",
      "            )\n",
      "            model = model.to(device)\n",
      "            print(model)\n",
      "            utils.print_parameters(model)\n",
      "            optimizer = optim.Adam(\n",
      "                model.parameters(),\n",
      "                lr=0.0001,\n",
      "                weight_decay=1e-5,\n",
      "                eps=1e-08,\n",
      "                amsgrad=False,\n",
      "            )\n",
      "            prep_fn = utils.prepare_minibatch\n",
      "        elif args.model == \"PTDeepCBOW\":\n",
      "            hidden_layer = 100\n",
      "            n_classes = len(train_dataset.vocab.t2i)\n",
      "            vocab, pretrained_vectors, embedding_dim = (\n",
      "                utils.create_vocabulary_and_embeddings(args.word_embeddings)\n",
      "            )\n",
      "            model = bow.PTDeepCBOW(\n",
      "                vocab_size=len(vocab.w2i),\n",
      "                embedding_dim=embedding_dim,\n",
      "                hidden_dim=hidden_layer,\n",
      "                output_dim=n_classes,\n",
      "                vocab=vocab,\n",
      "                pretrained_vectors=pretrained_vectors,\n",
      "                trainable_embeddings=args.trainable_embeddings,\n",
      "            )\n",
      "            model = model.to(device)\n",
      "            print(model)\n",
      "            utils.print_parameters(model)\n",
      "            optimizer = optim.Adam(\n",
      "                model.parameters(),\n",
      "                lr=0.0001,\n",
      "                weight_decay=1e-5,\n",
      "                eps=1e-08,\n",
      "                amsgrad=False,\n",
      "            )\n",
      "            prep_fn = utils.prepare_minibatch\n",
      "        elif args.model == \"LSTM\":\n",
      "            vocab, pretrained_vectors, embedding_dim = (\n",
      "                utils.create_vocabulary_and_embeddings(args.word_embeddings)\n",
      "            )\n",
      "            model = lstm.LSTMClassifier(\n",
      "                len(vocab.w2i), embedding_dim, 168, len(vocab.t2i), vocab\n",
      "            )\n",
      "\n",
      "            # copy pre-trained word vectors into embeddings table\n",
      "            with torch.no_grad():\n",
      "                model.embed.weight.data.copy_(torch.from_numpy(pretrained_vectors))\n",
      "                model.embed.weight.requires_grad = args.trainable_embeddings\n",
      "\n",
      "            print(model)\n",
      "            utils.print_parameters(model)\n",
      "\n",
      "            model = model.to(device)\n",
      "            optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
      "            prep_fn = utils.prepare_minibatch\n",
      "        elif args.model == \"TreeLSTM\":\n",
      "            vocab, pretrained_vectors, embedding_dim = (\n",
      "                utils.create_vocabulary_and_embeddings(args.word_embeddings)\n",
      "            )\n",
      "            model = lstm.TreeLSTMClassifier(\n",
      "                len(vocab.w2i),\n",
      "                embedding_dim,\n",
      "                168,\n",
      "                len(vocab.t2i),\n",
      "                vocab,\n",
      "                reduce_fn=lstm.TreeLSTMCell,\n",
      "            )\n",
      "\n",
      "            # copy pre-trained word vectors into embeddings table\n",
      "            with torch.no_grad():\n",
      "                model.embed.weight.data.copy_(torch.from_numpy(pretrained_vectors))\n",
      "                model.embed.weight.requires_grad = args.trainable_embeddings\n",
      "\n",
      "            print(model)\n",
      "            utils.print_parameters(model)\n",
      "\n",
      "            model = model.to(device)\n",
      "            optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
      "            prep_fn = utils.prepare_treelstm_minibatch\n",
      "        elif args.model == \"ChildSumTreeLSTM\":\n",
      "            vocab, pretrained_vectors, embedding_dim = (\n",
      "                utils.create_vocabulary_and_embeddings(args.word_embeddings)\n",
      "            )\n",
      "            model = lstm.TreeLSTMClassifier(\n",
      "                len(vocab.w2i),\n",
      "                embedding_dim,\n",
      "                168,\n",
      "                len(vocab.t2i),\n",
      "                vocab,\n",
      "                reduce_fn=lstm.ChildSumTreeLSTMCell,\n",
      "            )\n",
      "\n",
      "            # copy pre-trained word vectors into embeddings table\n",
      "            with torch.no_grad():\n",
      "                model.embed.weight.data.copy_(torch.from_numpy(pretrained_vectors))\n",
      "                model.embed.weight.requires_grad = args.trainable_embeddings\n",
      "\n",
      "            print(model)\n",
      "            utils.print_parameters(model)\n",
      "\n",
      "            model = model.to(device)\n",
      "            optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
      "            prep_fn = utils.prepare_treelstm_minibatch\n",
      "        else:\n",
      "            raise ValueError(f\"Model {args.model} not supported\")\n",
      "\n",
      "        train_epoch_losses, val_epoch_metrics, test_metrics, binned_metrics, max_epoch = (\n",
      "            utils.train_model(\n",
      "                model,\n",
      "                optimizer,\n",
      "                train_dataset,\n",
      "                dev_dataset,\n",
      "                test_dataset,\n",
      "                device=device,\n",
      "                max_epochs=args.max_epochs,\n",
      "                eval_every=args.eval_every,\n",
      "                batch_size=args.batch_size,\n",
      "                patience=args.patience,\n",
      "                batch_fn=utils.get_minibatch,\n",
      "                prep_fn=prep_fn,\n",
      "                eval_fn=utils.evaluate_metrics_extended_batch,\n",
      "            )\n",
      "        )\n",
      "\n",
      "        train_epoch_losses_list.append(train_epoch_losses)\n",
      "        val_epoch_metrics_list.append(val_epoch_metrics)\n",
      "        test_metrics_list.append(test_metrics)\n",
      "        binned_metrics_list.append(binned_metrics)\n",
      "        max_epochs.append(max_epoch)\n",
      "\n",
      "    # Calculate averages and standard deviations for all metrics\n",
      "    metrics_summary = {}\n",
      "\n",
      "    # Get all metric names from the last test metrics\n",
      "    metric_names = test_metrics_list[0].keys()\n",
      "\n",
      "    for metric in metric_names:\n",
      "        values = [test_metrics[metric] for test_metrics in test_metrics_list]\n",
      "        metrics_summary[f\"test_{metric}_mean\"] = np.mean(values)\n",
      "        metrics_summary[f\"test_{metric}_std\"] = np.std(values)\n",
      "\n",
      "    # For each bin, calculate mean and std of each metric\n",
      "    bins = set()\n",
      "    for binned_metrics in binned_metrics_list:\n",
      "        bins.update(binned_metrics.keys())\n",
      "    \n",
      "    for bin_name in sorted(bins):\n",
      "        for metric in metric_names:\n",
      "            # Get metric values for this bin across all seeds\n",
      "            values = []\n",
      "            for binned_metrics in binned_metrics_list:\n",
      "                if bin_name in binned_metrics:\n",
      "                    values.append(binned_metrics[bin_name]['metrics'][metric])\n",
      "            \n",
      "            # Calculate mean and std if we have values\n",
      "            if values:\n",
      "                metrics_summary[f\"{bin_name}_{metric}_mean\"] = np.mean(values)\n",
      "                metrics_summary[f\"{bin_name}_{metric}_std\"] = np.std(values)\n",
      "\n",
      "    # Print results\n",
      "    print(f\"Args: {args}\")\n",
      "    print(f\"Max epochs: {max_epochs}\")\n",
      "    print(f\"Last train loss: {round(train_epoch_losses_list[-1][-1], 2)}\")\n",
      "    print(f\"Last val metrics: {val_epoch_metrics_list[-1][-1]}\")\n",
      "\n",
      "    # Save detailed results for the current run\n",
      "    run_results = {\n",
      "        \"args\": vars(args),  # Convert argparse.Namespace to a dictionary\n",
      "        \"metrics_summary\": {\n",
      "            metric_name: round(value, 5)\n",
      "            for metric_name, value in metrics_summary.items()\n",
      "        },\n",
      "        \"max_epochs\": max_epochs,  # List of max epochs for each seed\n",
      "        \"train_epoch_losses_list\": train_epoch_losses_list,  # Losses for all epochs\n",
      "        \"val_epoch_metrics_list\": val_epoch_metrics_list,  # Validation metrics for all epochs\n",
      "    }\n",
      "\n",
      "    # File naming based on arguments\n",
      "    result_file = os.path.join(\n",
      "        results_dir,\n",
      "        f\"{args.model}_{args.word_embeddings}_{args.trainable_embeddings}_{args.supervise_nodes}.json\",\n",
      "    )\n",
      "\n",
      "    # Save as JSON\n",
      "    with open(result_file, \"w\") as f:\n",
      "        json.dump(run_results, f, indent=4)\n",
      "\n",
      "    print(f\"Results saved to {result_file}\")\n",
      "\n",
      "    return metrics_summary\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from train import main\n",
    "print(inspect.getsource(main))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
